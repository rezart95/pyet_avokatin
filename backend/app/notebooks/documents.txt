from langchain_core.documents import Document
from langchain_community.document_loaders import PyPDFLoader, PyMuPDFLoader

dir_loader = DirectoryLoader(
    "path/where/pdf/located",
    glob="**/*.pdf", #pattern to match files
    loader_cls=PyMuPDFLoader,
    show_progress=True
)

pdf_document=dir_loader.load()
pdf_documents()

#######################################################

## Embedding and vector store db
import numpy as np
from sentence_transformers import SentenceTransformer
import qdrant
import uuid
from typing import List, Dict, Any, Table
from sklearn.metrics.pairwise import cosine_similarity

class EmbeddingManager:
  """ Handles document embedding generation using Sentence Transformer"

  def __init__(self, model_name: str = "all-MiniLM-L6-v2"
      """
        initialise the embedding manager
      Args:
          model_name: Hugging Face model name for sentence embeddings
      """
      self.model_name = model_name
      self.model = None
      self._load_model()

    def _load_model(self):
      """ Load the Sentence Transformer Model"
    try:
        print(f"Loading embedding model: {self.model_name}")
        self.model = SentenceTransformer(self.model_name)
        print(f"Model loaded successfully. Embedding dimension: {self.model.get_sentence_embedding_dimension()}")
    except Exception as e:
        print(f"Error loading model {self.model_name}: {e}")
        raise

    def generate_embeddings(self, text: List[str]) -> np.ndarray:
        """ 
          Generate emebeddings for a list of texts
        Args:
          text
        """
        if not self.model:
          raise ValueError("Model Not Fount")
        print(f"Generating embeddings for {len(texts)} texts..")
        embeddings = self.model.encode(texts, show_progress_bar=True)
        print(f"Generated embeddings with shape: {embeddings.shape}")
        return embeddings

embedding_manager=EmbeddingManager()
embedding_manager

########################################

VECTOR STORE

class VectorStore:
  """ Manages document embeddings"

def __init__(self, collection_name: str = "pyet_avokatin", qdrant_path)
    """ Initialise the vector store"
    self.collection_name = collection_name
    self.client = None
    self.collection = None
    self.initialize_store()

def _initialise_store(self):
  """ Initialise connection to Qdrant """
  try:
      # Create client
      self.client = 
  
      # Get or create collection
      self.collection=
  
      print(f"Vector store connection successful: {self.collection_name}")
      print(f"Existing documents in collection: {self.collection.count()}")
  except Exception as e:
      print(f"Error initializing vectore store {e}")
      raise

  def add_documents(self, documents: List[Any], embeddings: npndarray):
    """ Add docuements and their embedding to the vector store 

      Args: documents:list of langchain documents, embeddings: Corresponding embeddings """

    if len(documents) != len(embeddings):
      raise ValueError("Number of documents must match the number of embeddings")

    print(f"Adding {len(documents)} documents to vectore store...")

    # Prepare data
    ids = []
    metadata = []
    document_text = []
    embedding_list = []

    for i, (doc, embedding) in enumerate(zip(documents, embeddings)):
      # Generate unique ID
      doc_id = f"doc_{uuid.uuid4().hex[:8]}_{i}"
      ids.append(doc_id)

      # Prepare metadata
      metadata = dict(doc.metadata)
      metadata['doc_index'] = i
      metadata['content_length'] = len(doc.page_content)
      metadatas.append(metadata)

      # Document content
      documents_text.append(doc.page_content)

    # Add to collection
    try:
        self.collection.add(
          ids=ids,
          embeddings=embeddings_list,
          metadatas=metadatas,
          documents=documents_text
        )
        print(f"Successfully added {len(documents)} cocuments to vectore store")
        print(f"Total documents in collection: {self.collection.count()}")
    except Exception as e:
      print("Error adding documents to vector store: {e}")

vectorestore = VectoreStore()

##############################################################
### Text splitting get into chunks

def split_documents(documents,chunk_size=1000,chunk_overlap=200):
    """Split documents into smaller chunks for better RAG performance"""
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=chunk_size,
        chunk_overlap=chunk_overlap,
        length_function=len,
        separators=["\n\n", "\n", " ", ""]
    )
    split_docs = text_splitter.split_documents(documents)
    print(f"Split {len(documents)} documents into {len(split_docs)} chunks")
    
    # Show example of a chunk
    if split_docs:
        print(f"\nExample chunk:")
        print(f"Content: {split_docs[0].page_content[:200]}...")
        print(f"Metadata: {split_docs[0].metadata}")
    
    return split_docs


chunks=split_documents(all_pdf_documents)
chunks


### Convert the text to embeddings
texts=[doc.page_content for doc in chunks]

## Generate the Embeddings

embeddings=embedding_manager.generate_embeddings(texts)

##store int he vector dtaabase
vectorstore.add_documents(chunks,embeddings)

